# ğŸš€ Logistic Data Lake â€” Airflow + MinIO + PostgreSQL + Soda Core

**Data Lake profissional modular**, desenvolvido para demonstrar uma arquitetura de ingestÃ£o, qualidade e processamento de dados seguindo o conceito **Medallion Architecture (RAW â†’ BRONZE â†’ SILVER â†’ GOLD)**.  
Totalmente executÃ¡vel em ambiente local com **Airflow + Docker Compose + MinIO + PostgreSQL**, e pronto para escalar em ambientes corporativos.

> âš ï¸ Este repositÃ³rio Ã© **privado** e destinado apenas para **demonstraÃ§Ã£o interna**. NÃ£o deve ser compartilhado ou clonado publicamente.

---

## ğŸ§  Objetivo

Este projeto foi construÃ­do como um **demo corporativo de Data Lakehouse**, combinando:
- ingestÃ£o incremental,
- processamento particionado em Parquet,
- governanÃ§a e qualidade de dados com **Soda Core**,
- orquestraÃ§Ã£o automatizada com **Apache Airflow**.

> Ideal para demonstraÃ§Ãµes tÃ©cnicas, POCs internas e ensino avanÃ§ado de Engenharia de Dados aplicada Ã  LogÃ­stica 5.0.

## ğŸ—ï¸ Arquitetura
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   RAW      â”‚  â† Dados brutos (CSV, JSON, APIs, etc.)
      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
            â”‚
     IngestÃ£o (Airflow + Python)
            â”‚
      â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
      â”‚  BRONZE    â”‚  â† PadronizaÃ§Ã£o, formataÃ§Ã£o, Parquet
      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
            â”‚
     Limpeza / ValidaÃ§Ã£o (Soda Core)
            â”‚
      â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
      â”‚  SILVER    â”‚  â† Dados refinados, prontos para modelagem
      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
            â”‚
     AgregaÃ§Ãµes / SQL puro (PostgreSQL)
            â”‚
      â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
      â”‚   GOLD     â”‚  â† Data Warehouse analÃ­tico
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

---

## âš™ï¸ Stack TÃ©cnica

| Componente | FunÃ§Ã£o | ObservaÃ§Ã£o |
|-------------|--------|-------------|
| **Apache Airflow 2.7+** | OrquestraÃ§Ã£o | LocalExecutor com DAGs modulares |
| **PostgreSQL** | Metadados e camada GOLD | Consultas SQL otimizadas |
| **MinIO (S3 local)** | Armazenamento RAW/BRONZE/SILVER | Via `s3fs` e `boto3` |
| **Parquet + PyArrow** | Formato de dados | Alta performance e compressÃ£o |
| **Soda Core** | Data Quality | Regras e monitoramento de qualidade |
| **Python** | ETL e lÃ³gica de negÃ³cio | Pandas, PyArrow, Faker, Boto3 |

---

## ğŸ§© Estrutura de Pastas
ğŸ“¦ Logistic_Datalake
â”£ ğŸ“‚ dags/ â†’ DAGs do Airflow (RAW, BRONZE, SILVER, GOLD, QA)
â”£ ğŸ“‚ scripts/ â†’ FunÃ§Ãµes auxiliares e ETLs
â”£ ğŸ“‚ data/ â†’ Dados particionados por camada (Parquet)
â”£ ğŸ“‚ soda/ â†’ Arquivos de configuraÃ§Ã£o e scans do Soda Core
â”£ ğŸ“‚ logs/ â†’ Logs do Airflow (ignorado no Git)
â”£ ğŸ“œ docker-compose.yml â†’ Infraestrutura local completa
â”£ ğŸ“œ requirements.txt â†’ DependÃªncias Python
â”£ ğŸ“œ .env â†’ VariÃ¡veis de ambiente (credenciais, paths)
â”— ğŸ“œ README.md 


---

## ğŸ§° Quick Start (Local)

### 1ï¸âƒ£ â€” Ativar ambiente local
```powershell
cd "C:\Users\Luis Camargo\Desktop\Logistic_Datalake"
.venv\Scripts\Activate.ps1

2ï¸âƒ£ â€” Subir a infraestrutura
docker-compose up -d

3ï¸âƒ£ â€” Acessar interfaces

| ServiÃ§o           | URL                                            | Login padrÃ£o                    |
| ----------------- | ---------------------------------------------- | ------------------------------- |
| **Airflow UI**    | [http://localhost:8080](http://localhost:8080) | `daxlog123` / `daxlog123`       |
| **MinIO Console** | [http://localhost:9001](http://localhost:9001) | `daxlog123` / `daxlog123`       |
| **PostgreSQL**    | localhost:5432                                 | DB: `gold_dw` / user: `airflow` |

ğŸ§® Qualidade de Dados â€” Soda Core

ApÃ³s a ingestÃ£o na camada BRONZE, executa-se validaÃ§Ãµes automÃ¡ticas:

ConsistÃªncia de schema

Campos nulos ou duplicados

Regras de negÃ³cio customizadas

Exemplo de execuÃ§Ã£o manual de scan:

soda scan -d postgres -c soda/config.yml soda/checks.yml

ğŸ“ˆ Futuro e ExtensÃµes

IntegraÃ§Ã£o com dbt-core para modelagem SQL moderna

Deploy remoto em ambientes corporativos (Azure, AWS, GCP)

AdiÃ§Ã£o de camadas Streaming (Kafka) e Monitoring (Grafana/Prometheus) 

ğŸ§¾ LicenÃ§a

MIT License â€” uso interno para demonstraÃ§Ã£o e aprendizado.
