<h1 align="center">ğŸ‘‹ OlÃ¡, eu sou <strong>Luis Camargo</strong></h1>
<h3 align="center">Especialista em LogÃ­stica e Engenharia de Dados</h3>

<p align="center">
  <a href="https://www.linkedin.com/in/luisespecialista/" target="_blank">
    <img src="https://img.shields.io/badge/LinkedIn-blue?logo=linkedin&logoColor=white" alt="LinkedIn"/>
  </a>
  <a href="mailto:especialista.luiscamargo@gmail.com">
    <img src="https://img.shields.io/badge/Email-especialista.luiscamargo%40gmail.com-red?logo=gmail&logoColor=white" alt="Email"/>
  </a>
  <a href="https://wa.me/5511940880735">
    <img src="https://img.shields.io/badge/WhatsApp-Contato-brightgreen?logo=whatsapp&logoColor=white" alt="WhatsApp"/>
  </a>
</p>

---

## ğŸš€ Logistic Data Lake â€” DemonstraÃ§Ã£o Interna

Este projeto Ã© uma **demonstraÃ§Ã£o corporativa de Data Lake**, baseado na arquitetura **Medallion (RAW â†’ BRONZE â†’ SILVER â†’ GOLD)**, totalmente executÃ¡vel em ambiente local com:

- **Airflow** (orquestraÃ§Ã£o e automaÃ§Ã£o de pipelines)
- **PostgreSQL** (metadados do Airflow + camada GOLD)
- **MinIO** (armazenamento RAW, BRONZE e SILVER em Parquet)
- **Soda Core** (monitoramento e validaÃ§Ã£o de qualidade de dados)
- **SQL puro e Python** para mÃ¡xima performance

> âš ï¸ Este repositÃ³rio Ã© **privado** e destinado apenas a demonstraÃ§Ã£o interna. NÃ£o deve ser compartilhado publicamente.

---

## ğŸ§© Objetivo

Mostrar **como projetar, validar e executar pipelines de Data Lake** corporativos, permitindo:

- IngestÃ£o incremental de dados brutos
- Processamento e padronizaÃ§Ã£o em Parquet
- Monitoramento de qualidade de dados com Soda Core
- TransformaÃ§Ãµes e agregaÃ§Ãµes em SQL puro
- OrquestraÃ§Ã£o de fluxo de dados com Airflow

---

## ğŸ—ï¸ Arquitetura
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   RAW      â”‚  â† Dados brutos (CSV, JSON, APIs, etc.)
      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
            â”‚
     IngestÃ£o (Airflow + Python)
            â”‚
      â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
      â”‚  BRONZE    â”‚  â† PadronizaÃ§Ã£o, formataÃ§Ã£o, Parquet
      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
            â”‚
     ValidaÃ§Ã£o (Soda Core)
            â”‚
      â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
      â”‚  SILVER    â”‚  â† Dados refinados e prontos para modelagem
      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
            â”‚
     AgregaÃ§Ãµes / SQL puro (PostgreSQL)
            â”‚
      â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
      â”‚   GOLD     â”‚  â† Data Warehouse analÃ­tico
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

      
---

## âš™ï¸ Stack TÃ©cnica

- **ğŸ“Š Dados & BI:** Parquet, PyArrow, Python (Pandas, NumPy), SQL  
- **âš™ï¸ Engenharia de Dados:** Airflow, PostgreSQL, MinIO, Docker, ETL  
- **ğŸ§  Data Quality:** Soda Core, validaÃ§Ãµes automÃ¡ticas de regras de negÃ³cio  
- **ğŸŒ AutomaÃ§Ã£o & Workflow:** Python, DAGs Airflow, integraÃ§Ã£o local com MinIO  
- **ğŸ“ˆ Performance LogÃ­stica:** Monitoramento de KPIs de SLA, custo e operaÃ§Ã£o  

---

## ğŸ“‚ Estrutura de Pastas

ğŸ“¦ Logistic_Datalake
â”£ ğŸ“‚ dags/ â†’ DAGs Airflow (RAW â†’ GOLD + QA)
â”£ ğŸ“‚ scripts/ â†’ FunÃ§Ãµes auxiliares e ETLs
â”£ ğŸ“‚ data/ â†’ Dados particionados por camada (Parquet)
â”£ ğŸ“‚ soda/ â†’ ConfiguraÃ§Ã£o e scans do Soda Core
â”£ ğŸ“‚ logs/ â†’ Logs do Airflow (nÃ£o versionados)
â”£ ğŸ“œ docker-compose.yml â†’ Infraestrutura local
â”£ ğŸ“œ requirements.txt â†’ DependÃªncias Python
â”£ ğŸ“œ .env â†’ VariÃ¡veis de ambiente (credenciais)
â”— ğŸ“œ README.md

